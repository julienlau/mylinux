#!/usr/bin/env bash
set -e

logdest=spark-submit-tmp-$(date '+%y%m%d')-$(date '+%H%M%S')-$$
mkfifo $logdest

echo "apache spark-submit overloaded by datahub $0"
if [ -z "${SPARK_HOME}" ]; then
  echo "SPARK_HOME variable must be set"
  exit 9
fi

# disable randomized hash for string in Python 3.3+
export PYTHONHASHSEED=0
exec "${SPARK_HOME}"/bin/spark-class org.apache.spark.deploy.SparkSubmit "$@" > ${logdest} 2>&1 &

errcount=0
ok=0
while IFS= read -r line < ${logdest}
do   
    echo "$line"    
    if [[ $line == *"pending reason: ImagePullBackOff"* ]]; then
        errcount=$(($errcount+1))
    elif [[ $line == *"termination reason: Error"* ]]; then
        errcount=$(($errcount+10))
    elif [[ $line == *"termination reason: Completed"* ]]; then
        ok=1
    fi
    if [[ $errcount -ge 5 ]]; then
        echo "Force exit"
        exit 2
    fi
done

wait

if [[ $ok -eq 0 ]]; then
    echo "Force exit due to not Completed"
    exit 3
fi
